{
	"pages": [
		{"title": "Admin", "text": "Admin Module\nArchitecture Overview\n \n\nAttributes\n\nCentralise project&#39;s data\nAccess point for the admin GUI\nManage an index of Questions and Replies\nThis index can be saved on disk (pickle protocol) in a first stage\nIn a later development, the use of an Elasticsearch database instead of a local file could be considered\n\n\n\n\nNB: Many parts of previous version of maestro&#39;s code can be reused.\n\nDocker Configuration\nENVIRONMENT VARIABLES\n\nMandatory\n\nRMQ_IP: RabbitMQ broker FQN or IP\nGIT_SERVER: GOGS FQN or IP\nGIT_DOMAIN: Client&#39;s domain name on GIT\nGIT_PROJECT: Client&#39;s project name\nTARGET: Pod&#39;s target (or branch) - ex: prod, test, ...\n\n\nOptional\n\nRMQ_USER: tbott\nRMQ_PASSWD: M0m3n773ch\nCOMMAND_QUEUE: tbott-admin-in    (Commands from GUI backend)\nREPLY_QUEUE: tbott-admin-out  (Replies to GUI backend)\nCOMMAND_TOPIC: $GIT_DOMAIN.$GIT_PROJECT.$TARGET\nENCODER_QUEUE: tbott-encoder\nRMQ_PORT: 5672\nLOG_LEVEL: INFO  \n\n\n\nVOLUMES\n\nlogs: mounted in /var/log/supervisor\ndata_dir: mounted in /opt/admin\n\nAdmin Master election\nMessaging architecture\n\n2 topic queues:\nQ for questions ($DOMAIN.$PROJECT.$TARGET.Q.#)\nA for answers  ($DOMAIN.$PROJECT.$TARGET.A.#)\nAll Admin modules see all questions and all answers\n\n\n\n \n\nState diagram\n \n\nAdmin GUI commands\n\nCommands are atomics → no need for multithreading\nCommands are adressed to multiple Pods\nOnly one pod, the master, reply to all commands\n\n\nMessaging architecture\n \n\nCommand processing\n \n\nFAISS request example:\n{&quot;requests&quot; : [&quot;create&quot;],\n &quot;create&quot; : [\n               {&quot;vector&quot; : numpy.array , &quot;id&quot; : 1},\n               {&quot;vector&quot; : numpy.array , &quot;id&quot; : 2}\n             ]\n} \nQuestions index example:\n{\n  &quot;git&quot;: {&quot;path&quot;: DOMAIN/PROJECT\n          &quot;branch&quot;: test,\n          &quot;commit&quot;: git_commit_number\n          }\n  &quot;references&quot;: {\n    ref_1: {&quot;ids&quot;: [id1, id2],\n            &quot;categories&quot;: {&quot;N1&quot;: &quot;n1_cat1&quot;, &quot;N2&quot;: &quot;n2_cat1&quot;},\n            &quot;reply&quot;: &quot;a reply...&quot;\n            }\n    ref_2: {&quot;ids&quot;: [id3],\n            &quot;categories&quot;: {&quot;N1&quot;: &quot;n1_cat2&quot;, &quot;N2&quot;: &quot;n2_cat2&quot;},\n            &quot;reply&quot;: &quot;another reply...&quot;\n            }\n  }\n  &quot;all_categories&quot;: [{&quot;N1&quot;: &quot;n1_cat1&quot;, &quot;N2&quot;: &quot;n2_cat1&quot;},\n                      {&quot;N1&quot;: &quot;n1_cat2&quot;, &quot;N2&quot;: &quot;n2_cat2&quot;}]\n  &quot;ids&quot;:{\n    id1: {\n        &quot;ref&quot;: ref_1,\n        &quot;question&quot;: &quot;a quetion&quot;,\n        &quot;augmented&quot;: false,\n        &quot;vector&quot;: np.array},\n    id2: {\n        &quot;ref&quot;: ref_1,\n        &quot;question&quot;: &quot;n1_cat, a quetion&quot;,\n        &quot;augmented&quot;: true,\n        &quot;vector&quot;: np.array},\n    id3: {\n        &quot;ref&quot;: ref_1,\n        &quot;question&quot;: &quot;n1_cat, n2_cat, a quetion&quot;,\n        &quot;augmented&quot;: true,\n        &quot;vector&quot;: np.array},\n    id4: {\n        &quot;ref&quot;: ref_2,\n        &quot;question&quot;: &quot;a 2nd quetion&quot;,\n        &quot;vector&quot;: np.array}\n    }\n}\n\nQuestions Index queries\n\nInternal requests from Find Reply module\nMultithreaded\nAccess the index through a Read/Write Lock with priority on Write\nRWLock lib: https://github.com/elarivie/pyReaderWriterLock\n\n\n\n \n\n", "tags": "", "url": "Admin/index.html"},
		{"title": "FAISS", "text": "FAISS Module\nAttributes\n\nBased on Facebook AI Similarity Search\nHeart of a fast search system\nAble to index litteraly billions of inputs\nOnly handle matrices, of any shapes\nAs simple as possible, to reduce risks.\nAs generic as possible, to be reusable for any kind of inputs\n\nArchitecture Overview\n \n\n  \n    \n      Entry Point:      \n        Uses Messaging lib\n        Internal communication only (Admin &amp; FindReply)\n        Based on ØMQ for fast exchanges\n        Multithreaded to serve mutliple requests\n        Receives requests with Numpy arrays\n      \n      \n      Manage Index:      \n        Handle 2 scenarios:\n          \n            Indexing requests\n            Search requests\n          \n        \n      \n      \n      Faiss Index:      \n        Protected by a Read/Write Lock\n        Priority given to write access\n        Index is saved in data_dir volume\n      \n    \n    \n  \n\n\nDocker Configuration\nENVIRONMENT VARIABLES\n\nMandatory\nNone\n\n\nOptional\nNB_MATCHING: number of matching results to return  ► High enougth to include the right wuestion, if it exists  ► Low enougth to not overcharge sentence matching module  ► default = _10_  \nLOG_LEVEL: INFO  \n\n\n\nVOLUMES\n\nlogs: mounted in /var/log/supervisor\ndata_dir: mounted in /opt/faiss\n\nIndexing procedure\nAs stated in Faiss FAQ only the add procedure is supported.Thus the delete and update procedures boils down to simply re-creating a new index.This imply that the list of vectors and the their associated id must be saved on disk.\nIndexing requests come from Admin module.A create request always comes alone.delete, add and update requests may come in any combinations.  \nCreating a new Index\nDeserialized create request example:  \n{&quot;requests&quot; : [&quot;create&quot;],\n &quot;create&quot; : [\n               {&quot;vector&quot; : numpy.array , &quot;id&quot; : 1},\n               {&quot;vector&quot; : numpy.array , &quot;id&quot; : 2}\n             ]\n} \n \n\nCreate a Flat index\nAdd the list of vectors and their IDs to the index\nSave the index on disk in $data_dir (cf. docker configuration)\nSave the list of vectors and their IDs on disk in $data_dir\nuse pickle protocol to save python dict with numpy arrays on disk\ncheck data_factory.prepare_data function in maestro for an example  \n\n\n\nCode snippet\nimport faiss\nimport numpy as np\n\nwith alock.gen_wlock():\n  xb = np.array(get_vector_list(request)) # create matrix\n  dim = xb.shape[1]                       # vector&#39;s dimension\n  tmp_index = faiss.IndexFlatL2(dim)      # Flat index without ids\n  ids = get_ids(request)\n  index = faiss.IndexIDMap(tmp_index)     # index with ids\n  index.add_with_ids(xb, ids)             # add vectors and ids\n\nUpdating the Index\ndelete, add and update requests may come in any combinations.To update an index:\n\nretrieve vectors and IDs from disk\nupdate as specified\ncreate a new index\nsave new index and new data on disk\n\nupdating request example:  \n{ &quot;requests&quot; : [&quot;add&quot;, &quot;update&quot;, &quot;delete&quot;],\n  &quot;add&quot; : [{&quot;vector&quot; : numpy.array , &quot;id&quot; : 10}],\n  &quot;update&quot;: [{&quot;vector&quot; : numpy.array , &quot;id&quot; : 11}],\n  &quot;delete&quot;: [{&quot;id&quot;: 1}, {&quot;id&quot;: 2}]\n} \n\nSearching procedure\nIndexing requests come from FindReply module.Return a matrix of shape 2 X Nb_Questions X $NB_MATCHING   \n\nshape 2 correspond to IDs and scores\nshape Nb_Questions  correspond to the number of questions to search\nshape $NB_MATCHING correspond to the number of matchings to return(defined in docker config)\n\n \nsearch request example:  \n{ &quot;requests&quot; : [&quot;search&quot;],\n  &quot;search&quot; : [numpy.array , numpy.array]\n} \n\n \nCode snippet\n#K=NB_MATCHING : Number of results returned by query (default = 10)\nwith alock.gen_rlock():\n  S, I = index.search(q_vectors, K) # S: scores, I: indexes\n  res = np.array([I, S])            # Transform S and I in a matrix\n  send(res)\n\n", "tags": "", "url": "FAISS/index.html"},
		{"title": "Find Reply", "text": "Find Reply Module\nArchitecture Overview\n \n\nAttributes\n\nAggregate content from different sources\nMost of its time spent in waiting a reply from those sources\nMust use mutlithreading to serve RPC requests from RASA to optimize performances\n\nDocker Configuration\nENVIRONMENT VARIABLES\n\nMandatory\n\nRMQ_IP: RabbitMQ broker FQN or IP  \n\n\nOptional\n\nNB_INSTANCES: number of allocated threads  ► limits the number of questions in parallel  ► defines service level capacities (dimensioning rule)  ► default =  5  \nRMQ_USER: tbott\nRMQ_PASSWD: M0m3n773ch\nQUESTIONS_QUEUE: tbott-rpc    (Questions from RASA)\nENCODER_QUEUE: tbott-encoder  (Bert encoder queue)\nMATCHING_QUEUE: tbott-matching (Sentence matching queue)\nRMQ_PORT: 5672\nLOG_LEVEL: INFO  \n\n\n\nVOLUMES\n\nlogs: mounted in /var/log/supervisor\n\nCommunications\nmost of the code can be picked from the previous version of Maestro   \nTwo type of requests\nTo be compatible with the existing  \n\n&quot;question&quot; : questions coming from RASA\n&quot;analyse_sentence&quot; : From Analyser  \ncan be implemented in a second or third stage ~ not mandatory ~  \nreuse codes from the previous Maestro version  \nneeds to generate a list of questions with one and two words missings  \nnumber of free threads must be set to 0, or heavily decreased, to save resources\n\n\n\ncode trails\nLine _51_ of ai_worker.py:\nself.worker = Worker(ip, user, passwd, channel, self.handle_message)\n\nLine _57_ of ai_worker.py:\ndef handle_message(self, msg):\n  &quot;&quot;&quot;\n    Callback to handle messages\n  &quot;&quot;&quot;\n  LOGGER.debug(&#39;Received: %s&#39;, msg)\n  try:\n    request = json.loads(msg)\n    if request[&#39;type&#39;] == &quot;question&quot;:\n      full_answer = self.find_reply(request)\n      return json.dumps(full_answer)\n    elif request[&#39;type&#39;] == &quot;analyse_sentence&quot;:\n      full_answer = self.analyse_sentence(request[&#39;content&#39;], request[&#39;categories&#39;], request[&#39;reference&#39;])\n      return json.dumps(full_answer)\n\n...\n\nStep 1: contexts\nNB: only if message is a &quot;question&quot; (ie comes from RASA)\n\nget all possible categories (or contexts) from Admin.\nextend question list with all possible contexts\n\nStep 2: vectors\nSend question list to rabbitMQ $ENCODER_QUEUE using the Messaging library.\nRequest example:\n{ \n  &quot;request&quot;: &quot;encode&quot;,\n  &quot;content&quot;: [&quot;original_question&quot;, &quot;extended_question1&quot;, &quot;...&quot;]\n}\n\nReply example:\n{ \n  &quot;encoded&quot;: [numpy.array, numpy.array, numpy.array]\n}\n\nStep 3 : candidates\nGet possible candidates from FAISS module (127.0.0.1:14000)  \nSearch request example:  \n{ &quot;requests&quot; : [&quot;search&quot;],\n  &quot;search&quot; : [numpy.array , numpy.array]\n} \n\nFAISS reply example:  \n{ &quot;result&quot; : numpy.array }\n\n \nThe FAISS module send back a matrix of size 2 x nb_questions x nb_results :\n\n2 : IDs and scores (in this order)\nnb_questions: number of questions sent to FAISS\nnb_results: number of results per question (top k results)\n\n \nIf the question comes from RASA (ie message type = &quot;question&quot;), then all the questions are derivated from the original question.Which means that the results must be consolidated into only one list of IDs.This shouldbe done via Numpy as python&#39;s loops are quite time consuming.\nCode snippet:\nres = search_faiss(q_vectors)\nI = res[0]\nS = res[1]\nNQ = I.shape[0]              # Number of questions\nK = I.shape[1]               # Number of results per question\nI_ = np.reshape(I, NQ * K)   # reshape matrice into a list\nS_ = np.reshape(S, NQ * K)   # idem\nres_ids = np.argsort(S_)     # indexes of the sorted scores\n#first occurence of each result\n_, i = np.unique(I_[res_ids], return_index=True)\nI_ = I_[i]                   # keep uniq ids\nS_ = S_[i]                   # Keep uniq scores\nfinal_idx = np.argsort(S_)   # Indexes of the sorted scores\nfinal_idx = final_idx[:K]     # Keep only the K first results\nfinal_ids = I_[final_idx]\nfinal_scores = S_[final_idx]\nreturn final_ids, final_scores\n\nStep 4 : Questions to match\n\nGet from Admin questions and replies associated to the list of candidates IDs.  \nBuild a list of paired questions.\nAugmented questions must be paired only with questions from the same categories/contexts\n\n\n\nStep 5 : Best match\n\nSend the list of questions pairs to rabbitMQ $MATCHING_QUEUE using the Messaging library.\nBuild the final reply with the best match and other candidates\nchuncks of previous version of Maestro can be reused to speed up devs\n\n\n\nTesting procedure\n\n  \n    \n      \n        &quot;backdoor&quot; to test question directly from admin consol\n        Uses Messaging lib\n        Internal communication only (Admin)\n      \n    \n    \n  \n\n", "tags": "", "url": "FindReply/index.html"},
		{"title": "Maestro Architecture", "text": "Current architecture&#39;s limitations\n\nCreated for one big client, not in line with our current needs\nRequires re-training for every new data added\nTraining process hard to fully automized\nCategories and sub-categories are hard to define correctly\nMany problems arise when using datasets with overlapping meanings\n\nGuide lines for an evolution\n\nAvoid specific trainings whenever it&#39;s possible\nDo not require huge amount of data to start working\nBy default, no data leaks between projects\nUse categories as contextual helps instead of arbitrary silosEx: The question How many dead during the landing ?in the absolute, is not equal toHow many dead during the landing of the bay of pigs ?norHow many dead during the landing of the D-Day ?  \n\nArchitecture Overview\nContext\n \n\nRasa(s)\nBecause each project has its own instance of Maestro, we must also have multiple instances of Rasa, one per Maestro  \n\nNeural Networks\nAs explained in Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks paper, while pretrained BERT model perform poorly for sentence embeddings, BERT models fine-tuned on sentence matching are slow and resource hungry.  \nSo to build an efficient search engine we must use two different models,\n\none for sentence encoding that will be fed to an efficient search index such as FAISS to quickly output candidates,\nand another one for sentence matching to be able to make the difference between a best match and a true match \n\n\nSentence Encoder\n\nEncode sentence as a contextual vector\nex: Universal Sentence Encoder Multilingual from Google\nUsed in Faiss Index to find best candidates\nTrained only once\nFollows SOTA improvements\n\nSentence Matching\n\nTells if two sentences are the same or not\nProvides  relevance score\nDo not use sentence encoder&#39;s vectors\nTrained only once\nFollows SOTA improvements\n\nMaestro\n\nMaestro is split in 3 parts for maintainability and efficiency.\nAll 3 containers are grouped within the same Pod\nA Pod is dedicated to a Target (ex: prod or test) attached to a Project within a DomainIn this doc referred as Domain.Project.Target\nFor redundancy and scalability, multiple Pods can share the same Domain.Project.env\nCommunications within a Pod is done through ZeroMQ on 127.0.0.1Containers within a Pod share the same network namespace, enabling the use of localhost address.ZeroMQ is fast, flexible and is indifferent to the startup order of clients and servers.It will more or less act as an IPC within the Pod\nMessages between containers will use JSON format containing Numpy arrays.Exemple of numpy serialisation with ZeroMQ: https://pyzmq.readthedocs.io/en/latest/serialization.html\n\nModules description\n\nFAISS\nFindReply\nAdmin\n\n", "tags": "", "url": "index.html"}
	]
}
