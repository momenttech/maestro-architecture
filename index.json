{
	"pages": [
		{"title": "Admin", "text": "Admin Module\nArchitecture Overview\n \n\nAttributes\n\nCentralise project&#39;s data\nAccess point for the admin GUI\nManage an index of Questions and Replies\nThis index can be saved on disk (pickle protocol) in a first stage\nIn a later development, the use of an Elasticsearch database instead of a local file could be considered\n\n\n\n\nNB: Many parts of previous version of maestro&#39;s code can be reused.\n\nDocker Configuration\nENVIRONMENT VARIABLES\n\nMandatory\n\nRMQ_IP: RabbitMQ broker FQN or IP\nGIT_SERVER: GOGS FQN or IP\nGIT_BRANCH: Kingdom branch\nDOMAIN: Client&#39;s domain name on GIT\nPROJECT: Client&#39;s project name\nTARGET: Pod&#39;s target (or branch) - ex: prod, test, ...\n\n\nOptional\n\nRMQ_USER: ...\nRMQ_PASSWD: ...\nRMQ_TOPIC: $GIT_DOMAIN.$GIT_PROJECT.$TARGET\nRMQ_EXCHANGE: amq.topic\nENCODER_QUEUE: tbott-encoder\nRMQ_PORT: 5672\nLOG_LEVEL: INFO\n\n\n\nVOLUMES\n\nlogs: mounted in /var/log/supervisor\ndata_dir: mounted in /opt/admin\n\nAdmin Master election\nMessaging architecture\n\n2 topic queues:\nQ for questions ($DOMAIN.$PROJECT.$TARGET.Q.#)\nA for answers  ($DOMAIN.$PROJECT.$TARGET.A.#)\nAll Admin modules see all questions and all answers\n\n\n\n \n\nState diagram\n \n\nAdmin GUI commands\n\nCommands are atomics → no need for multithreading\nCommands are adressed to multiple Pods\nOnly one pod, the master, reply to all commands\n\n\nMessaging architecture\n \n\nCommand processing\n \n\nMessages from Admin GUI:\n\nNormaly, only an update request is necessary:{&quot;request&quot; : &quot;update&quot;}\n\n\nIf needed you can pass optional parameters to change the kingdom&#39;s branch or the project&#39;s branch:{&quot;request&quot;: &quot;update&quot;,\n &quot;kingdom&quot;: &quot;training&quot;,\n &quot;branch&quot;: &quot;v6&quot;\n }\n\n\nA kingdom branch change may be needed when changing the access to other Domains/Projects\nA project branch change may be trigerred by a rollback or a testing procedure\n\n\n\nFAISS request example:\n{&quot;requests&quot; : [&quot;create&quot;],\n &quot;create&quot; : [\n               {&quot;vector&quot; : numpy.array , &quot;id&quot; : 1},\n               {&quot;vector&quot; : numpy.array , &quot;id&quot; : 2}\n             ]\n} \n\nQuestions index example:\n{\n  &quot;git&quot;: {&quot;path&quot;: DOMAIN/PROJECT\n          &quot;branch&quot;: test,\n          &quot;commit&quot;: git_commit_number\n          }\n  &quot;references&quot;: {\n    ref_1: {&quot;ids&quot;: [id1, id2],\n            &quot;categories&quot;: {&quot;N1&quot;: &quot;n1_cat1&quot;, &quot;N2&quot;: &quot;n2_cat1&quot;},\n            &quot;reply&quot;: &quot;a reply...&quot;\n            }\n    ref_2: {&quot;ids&quot;: [id3],\n            &quot;categories&quot;: {&quot;N1&quot;: &quot;n1_cat2&quot;, &quot;N2&quot;: &quot;n2_cat2&quot;},\n            &quot;reply&quot;: &quot;another reply...&quot;\n            }\n  }\n  &quot;all_categories&quot;: [{&quot;N1&quot;: &quot;n1_cat1&quot;, &quot;N2&quot;: &quot;n2_cat1&quot;},\n                      {&quot;N1&quot;: &quot;n1_cat2&quot;, &quot;N2&quot;: &quot;n2_cat2&quot;}]\n  &quot;ids&quot;:{\n    id1: {\n        &quot;ref&quot;: ref_1,\n        &quot;question&quot;: &quot;a quetion&quot;,\n        &quot;augmented&quot;: false,\n        &quot;vector&quot;: np.array},\n    id2: {\n        &quot;ref&quot;: ref_1,\n        &quot;question&quot;: &quot;n1_cat, a quetion&quot;,\n        &quot;augmented&quot;: true,\n        &quot;vector&quot;: np.array},\n    id3: {\n        &quot;ref&quot;: ref_1,\n        &quot;question&quot;: &quot;n1_cat, n2_cat, a quetion&quot;,\n        &quot;augmented&quot;: true,\n        &quot;vector&quot;: np.array},\n    id4: {\n        &quot;ref&quot;: ref_2,\n        &quot;question&quot;: &quot;a 2nd quetion&quot;,\n        &quot;vector&quot;: np.array}\n    }\n}\n\nQuestions Index queries\n\nInternal requests from Find Reply module\nMultithreaded\nAccess the index through a Read/Write Lock with priority on Write\n\nRWLock lib: https://github.com/elarivie/pyReaderWriterLock\n\n\nExemple of context request:\n\nrequest: { &quot;request&quot;: &quot;get_contexts&quot;}\n\n\nreply: { \n &quot;request&quot;: &quot;get_contexts&quot;,\n &quot;result&quot;:  [{&quot;cat1&quot;: [{&quot;cat1&quot;: &quot;DDAY&quot;}]},  \n             {&quot;cat1_cat2&quot;: [{&quot;cat1&quot;: &quot;D-DAY&quot;, &quot;cat2&quot;: &quot;Bilan&quot;},  \n                            {&quot;cat1&quot;: &quot;D-DAY&quot;, &quot;cat2&quot;: &quot;Cimetières&quot;}\n                           ]}]\n }\n\n\n\n\nExemple of info request:\n\nrequest: { &quot;request&quot;: &quot;get_candidates&quot;,\n   &quot;ids&quot;: [1, 2, 3]}\n\n\nreply: { \n &quot;request&quot;: &quot;get_candidates&quot;,\n &quot;result&quot;:  [{&quot;id&quot;: 1, &quot;ref&quot;: &quot;DDAY/dday/20&quot;, \n              &quot;question&quot;: &quot;Quelles sont les plages du débarquement ?&quot;, \n              &quot;augmented&quot;: false, \n              &quot;reply&quot;: &quot;Les 5 plages du débarquement sont ...&quot;, \n              &quot;cat1&quot;: &quot;D-DAY&quot;, &quot;cat2&quot;: &quot;Lieux de Batailles&quot;}, \n             {&quot;id&quot;: 2, &quot;ref&quot;: &quot;DDAY/dday/20&quot;, \n              &quot;question&quot;: &quot;D-DAY, Quelles sont les plages du débarquement ?&quot;, \n              &quot;augmented&quot;: true, \n              &quot;reply&quot;: &quot;Les 5 plages du débarquement sont ...&quot;, \n              &quot;cat1&quot;: &quot;D-DAY&quot;, &quot;cat2&quot;: &quot;Lieux de Batailles&quot;},\n             {&quot;id&quot;: 3, &quot;ref&quot;: &quot;DDAY/dday/20&quot;, \n              &quot;question&quot;: &quot;D-DAY, Lieux de Batailles, Quelles sont les plages du débarquement ?&quot;, \n              &quot;augmented&quot;: true, \n              &quot;reply&quot;: &quot;Les 5 plages du débarquement sont ...&quot;, &quot;cat1&quot;: &quot;D-DAY&quot;, &quot;cat2&quot;: &quot;Lieux de Batailles&quot;}\n             ]}\n\n \n\n\n\n\n\n", "tags": "", "url": "Admin/index.html"},
		{"title": "FAISS", "text": "FAISS Module\nAttributes\n\nBased on Facebook AI Similarity Search\nHeart of a fast search system\nAble to index litteraly billions of inputs\nOnly handle matrices, of any shapes\nAs simple as possible, to reduce risks.\nAs generic as possible, to be reusable for any kind of inputs\n\nArchitecture Overview\n \n\n  \n    \n      Entry Point:      \n        Uses Messaging lib\n        Internal communication only (Admin &amp; FindReply)\n        Based on ØMQ for fast exchanges\n        Multithreaded to serve mutliple requests\n        Receives requests with Numpy arrays\n      \n      \n      Manage Index:      \n        Handle 2 scenarios:\n          \n            Indexing requests\n            Search requests\n          \n        \n      \n      \n      Faiss Index:      \n        Protected by a Read/Write Lock\n        Priority given to write access\n        Index is saved in data_dir volume\n      \n    \n    \n  \n\n\nDocker Configuration\nENVIRONMENT VARIABLES\n\nMandatory\nNone\n\n\nOptional\nNB_MATCHING: number of matching results to return  ► High enougth to include the right wuestion, if it exists  ► Low enougth to not overcharge sentence matching module  ► default = _10_  \nLOG_LEVEL: INFO  \n\n\n\nVOLUMES\n\nlogs: mounted in /var/log/supervisor\ndata_dir: mounted in /opt/faiss\n\nIndexing procedure\nAs stated in Faiss FAQ only the add procedure is supported.Thus the delete and update procedures boils down to simply re-creating a new index.This imply that the list of vectors and the their associated id must be saved on disk.\nIndexing requests come from Admin module.A create request always comes alone.delete, add and update requests may come in any combinations.  \nCreating a new Index\nDeserialized create request example:  \n{&quot;requests&quot; : [&quot;create&quot;],\n &quot;create&quot; : [\n               {&quot;vector&quot; : numpy.array , &quot;id&quot; : 1},\n               {&quot;vector&quot; : numpy.array , &quot;id&quot; : 2}\n             ]\n} \n \n\nCreate a Flat index\nAdd the list of vectors and their IDs to the index\nSave the index on disk in $data_dir (cf. docker configuration)\nSave the list of vectors and their IDs on disk in $data_dir\nuse pickle protocol to save python dict with numpy arrays on disk\ncheck data_factory.prepare_data function in maestro for an example  \n\n\n\nCode snippet\nimport faiss\nimport numpy as np\n\nwith alock.gen_wlock():\n  xb = np.array(get_vector_list(request)) # create matrix\n  dim = xb.shape[1]                       # vector&#39;s dimension\n  tmp_index = faiss.IndexFlatL2(dim)      # Flat index without ids\n  ids = get_ids(request)\n  index = faiss.IndexIDMap(tmp_index)     # index with ids\n  index.add_with_ids(xb, ids)             # add vectors and ids\n\nUpdating the Index\ndelete, add and update requests may come in any combinations.To update an index:\n\nretrieve vectors and IDs from disk\nupdate as specified\ncreate a new index\nsave new index and new data on disk\n\nupdating request example:  \n{ &quot;requests&quot; : [&quot;add&quot;, &quot;update&quot;, &quot;delete&quot;],\n  &quot;add&quot; : [{&quot;vector&quot; : numpy.array , &quot;id&quot; : 10}],\n  &quot;update&quot;: [{&quot;vector&quot; : numpy.array , &quot;id&quot; : 11}],\n  &quot;delete&quot;: [{&quot;id&quot;: 1}, {&quot;id&quot;: 2}]\n} \n\nSearching procedure\nIndexing requests come from FindReply module.Return a matrix of shape 2 X Nb_Questions X $NB_MATCHING   \n\nshape 2 correspond to IDs and scores\nshape Nb_Questions  correspond to the number of questions to search\nshape $NB_MATCHING correspond to the number of matchings to return(defined in docker config)\n\n \nsearch request example:  \n{ &quot;requests&quot; : [&quot;search&quot;],\n  &quot;search&quot; : [numpy.array , numpy.array]\n} \n\n \nCode snippet\n#K=NB_MATCHING : Number of results returned by query (default = 10)\nwith alock.gen_rlock():\n  S, I = index.search(q_vectors, K) # S: scores, I: indexes\n  res = np.array([I, S])            # Transform S and I in a matrix\n  send(res)\n\n", "tags": "", "url": "FAISS/index.html"},
		{"title": "Find Reply", "text": "Find Reply Module\nArchitecture Overview\n \n\nAttributes\n\nAggregate content from different sources\nMost of its time spent in waiting a reply from those sources\nMust use mutlithreading to serve RPC requests from RASA to optimize performances\n\nDocker Configuration\nENVIRONMENT VARIABLES\n\nMandatory\n\nRMQ_IP: RabbitMQ broker FQN or IP  \nQUESTIONS_QUEUE: Questions from RASA  (Ex: tbott-rpc_.dom.proj_)\n\n\nOptional\n\nNB_INSTANCES: number of allocated threads  ► limits the number of questions in parallel  ► defines service level capacities (dimensioning rule)  ► default =  5  \nRMQ_USER: ...\nRMQ_PASSWD: ...\nENCODER_QUEUE: tbott-encoder  (Bert encoder queue)\nMATCHING_QUEUE: tbott-matching (Sentence matching queue)\nRMQ_PORT: 5672\nLOG_LEVEL: INFO  \n\n\n\nVOLUMES\n\nlogs: mounted in /var/log/supervisor\n\nCommunications\nmost of the code can be picked from the previous version of Maestro   \nTwo type of requests\nTo be compatible with the existing  \n\n&quot;question&quot; : questions coming from RASA\n&quot;analyse_sentence&quot; : From Analyser  \ncan be implemented in a second or third stage ~ not mandatory ~  \nreuse codes from the previous Maestro version  \nneeds to generate a list of questions with one and two words missings  \nnumber of free threads must be set to 0, or heavily decreased, to save resources\n\n\n\ncode trails\nLine _51_ of ai_worker.py:\nself.worker = Worker(ip, user, passwd, channel, self.handle_message)\n\nLine _57_ of ai_worker.py:\ndef handle_message(self, msg):\n  &quot;&quot;&quot;\n    Callback to handle messages\n  &quot;&quot;&quot;\n  LOGGER.debug(&#39;Received: %s&#39;, msg)\n  try:\n    request = json.loads(msg)\n    if request[&#39;type&#39;] == &quot;question&quot;:\n      full_answer = self.find_reply(request)\n      return json.dumps(full_answer)\n    elif request[&#39;type&#39;] == &quot;analyse_sentence&quot;:\n      full_answer = self.analyse_sentence(request[&#39;content&#39;], request[&#39;categories&#39;], request[&#39;reference&#39;])\n      return json.dumps(full_answer)\n\n...\n\nStep 1: contexts\nNB: only if message is a &quot;question&quot; (ie comes from RASA)\n\nget all possible categories (or contexts) from Admin.\n\nrequest:\n { &quot;request&quot;: &quot;get_contexts&quot;, , &quot;tag&quot;: &quot;prev_tag&quot;, &quot;content&quot;: &quot;cat1&quot;}\n\n\ntag: the tag given in the previous request&#39;s result.Set to None if it doesn&#39;t exist.  \ncontent: optional. It can be used to filter categories if one is provided in the question. You can omit it if not needed.\n\n\nreply:\n { \n &quot;request&quot;: &quot;get_contexts&quot;,\n &quot;result&quot;:  {\n     &quot;tag&quot;: &quot;update_tag&quot;,\n     &quot;dictionary&quot;: [&quot;word_a: word_b&quot;, &quot;word_x: word_y&quot;],\n     &quot;contexts&quot;: [{&quot;cat1&quot;: [{&quot;cat1&quot;: &quot;DDAY&quot;}]},  \n                          {&quot;cat1_cat2&quot;: [{&quot;cat1&quot;: &quot;D-DAY&quot;, &quot;cat2&quot;: &quot;Bilan&quot;},  \n                                         {&quot;cat1&quot;: &quot;D-DAY&quot;, &quot;cat2&quot;: &quot;Cimetières&quot;}]}\n                         ]\n }\n\n\ntag: a tag to track updates. If the tag in the request in equal to the last update tag then dictionary and contexts wil be empty and previous dictionary and contexts should be used.\ndictionary: a dictionary to pass to wordtools\ncontexts: list of all contexts\n\n\n\n\nextend question list with all possible contexts  \nThe list of questions must be preprocessed with the library wordtools 4.0 or higher:   from wordtools.text_processor import Document\n ...\n doc = Document(&#39;&#39;, dictionary=dictionary)\n processed = []\n for q in questions:\n   doc.text = q\n   doc.clean()\n   doc.apply_dictionary()\n   if auto_correct:\n     doc.auto_correct()\n   processed.append(doc.text)\n\n\n\n\n\nStep 2: vectors\nSend question list to rabbitMQ $ENCODER_QUEUE using the Messaging library.\nRequest example:\n{ \n  &quot;request&quot;: &quot;encode&quot;,\n  &quot;content&quot;: [&quot;original_question&quot;, &quot;extended_question1&quot;, &quot;...&quot;]\n}\n\nReply example:\n{ \n  &quot;result&quot;: [numpy.array, numpy.array, numpy.array]\n}\n\nStep 3 : candidates\nGet possible candidates from FAISS module (127.0.0.1:14000)  \nSearch request example:  \n{ &quot;requests&quot; : [&quot;search&quot;],\n  &quot;search&quot; : [numpy.array , numpy.array]\n} \n\nFAISS reply example:  \n{ &quot;result&quot; : numpy.array }\n\n \nThe FAISS module send back a matrix of size 2 x nb_questions x nb_results :\n\n2 : IDs and scores (in this order)\nnb_questions: number of questions sent to FAISS\nnb_results: number of results per question (top k results)\n\n \nIf the question comes from RASA (ie message type = &quot;question&quot;), then all the questions are derivated from the original question.Which means that the results must be consolidated into only one list of IDs.This shouldbe done via Numpy as python&#39;s loops are quite time consuming.\nCode snippet:\nres = search_faiss(q_vectors)\nI = res[0]\nS = res[1]\nNQ = I.shape[0]              # Number of questions\nK = I.shape[1]               # Number of results per question\nI_ = np.reshape(I, NQ * K)   # reshape matrice into a list\nS_ = np.reshape(S, NQ * K)   # idem\nres_ids = np.argsort(S_)     # indexes of the sorted scores\n#first occurence of each result\n_, i = np.unique(I_[res_ids], return_index=True)\nI_ = I_[i]                   # keep uniq ids\nS_ = S_[i]                   # Keep uniq scores\nfinal_idx = np.argsort(S_)   # Indexes of the sorted scores\nfinal_idx = final_idx[:K]     # Keep only the K first results\nfinal_ids = I_[final_idx]\nfinal_scores = S_[final_idx]\nreturn final_ids, final_scores\n\nStep 4 : Questions to match\n\nGet from Admin questions and replies associated to the list of candidates IDs.  \nrequest: { &quot;request&quot;: &quot;get_candidates&quot;,\n   &quot;content&quot;: [1, 2, 3]}\n\n\nreply: { \n &quot;request&quot;: &quot;get_candidates&quot;,\n &quot;result&quot;:  [{&quot;id&quot;: 1, &quot;ref&quot;: &quot;DDAY/dday/20&quot;, \n              &quot;question&quot;: &quot;Quelles sont les plages du débarquement ?&quot;, \n              &quot;augmented&quot;: false, \n              &quot;reply&quot;: &quot;Les 5 plages du débarquement sont ...&quot;, \n              &quot;cat1&quot;: &quot;D-DAY&quot;, &quot;cat2&quot;: &quot;Lieux de Batailles&quot;}, \n             {&quot;id&quot;: 2, &quot;ref&quot;: &quot;DDAY/dday/20&quot;, \n              &quot;question&quot;: &quot;D-DAY, Quelles sont les plages du débarquement ?&quot;, \n              &quot;augmented&quot;: true, \n              &quot;reply&quot;: &quot;Les 5 plages du débarquement sont ...&quot;, \n              &quot;cat1&quot;: &quot;D-DAY&quot;, &quot;cat2&quot;: &quot;Lieux de Batailles&quot;},\n             {&quot;id&quot;: 3, &quot;ref&quot;: &quot;DDAY/dday/20&quot;, \n              &quot;question&quot;: &quot;D-DAY, Lieux de Batailles, Quelles sont les plages du débarquement ?&quot;, \n              &quot;augmented&quot;: true, \n              &quot;reply&quot;: &quot;Les 5 plages du débarquement sont ...&quot;, &quot;cat1&quot;: &quot;D-DAY&quot;, &quot;cat2&quot;: &quot;Lieux de Batailles&quot;}\n             ]}\n\n\n\n\nBuild a list of paired questions.\nAugmented questions must be paired only with questions from the same categories/contexts\n\n\n\nStep 5 : Best match\nStarting with LaBSE encoder, cosine similartiry measures (provided by FAISS module) can be directly used as a matching score if:\n\nscore is &gt;= 0.90 for a question with more than 6 words\nscore is &gt;= 0.75 for a question with less than 7 words\n\n\nThis shortcut will improve latencies without sacrifying accuracy.\n\n\nSend the list of questions pairs to rabbitMQ $MATCHING_QUEUE using the Messaging library.\nBuild the final reply with the best match and other candidates\nchuncks of previous version of Maestro can be reused to speed up devs\n\n\n\nTesting procedure\n\n  \n    \n      \n        &quot;backdoor&quot; to test question directly from admin consol\n        Uses Messaging lib\n        Internal communication only (Admin)\n      \n    \n    \n  \n\n", "tags": "", "url": "FindReply/index.html"},
		{"title": "Messaging", "text": "Messaging details\nIntra Pod Messaging\nCommunication beetween elements of a pod is done via zeromq lib from Messaging package version 3.\nInstallation\npip3 install git+http://gitlab.mmtt.fr/AI/Messaging.git@3.0\nThe client\nThe client implement the Lazy Pirate pattern, a client-side reliability algorithme.Serialization and deserialization is done by the library and support numpy arrays.\nParameters:\n\nserver: remote address, should be 127.0.0.1\nport: remote port\nprotocol: Optional, default is tcp\n\nSimple exemple:  \nfrom Messaging import zeromq\nimport numpy as np\n\nclient = zeromq.LazyClient(&#39;127.0.0.1&#39;, 65000)\ndata = {&#39;request&#39;: &#39;inverse array&#39;, \n        &#39;content&#39;: np.array([[1, 2, 3],\n                             [4, 5, 6]])\n       }\n\nres = client.send(data)\nprint res\n\n&gt;&gt;&gt; {&#39;request&#39;: &#39;inverse array&#39;, \n     &#39;content&#39;: array([[-1, -2, -3], \n                       [-4, -5, -6]])\n    }\n\nThe Worker\nThe worker is multi-threaded, default is 5 threads.\nParameters:\n\ncallback: call function of the worker\nserver: address to bind, should be 127.0.0.1\nport: port to bind\nprotocol: Optional, default is tcp\nnb_workers: Optional, number of threads to use. Default is 5.\n\nSimple exemple:\nfrom time import sleep\nfrom Messaging import zeromq\nimport numpy as np\n\ndef inverse(data):\n    data[&#39;array&#39;] = -data[&#39;array&#39;]\n    return data\n\nworkers = zeromq.ThreadedWorker(inverse, &quot;127.0.0.1&quot;, 65000, nb_workers=2)\nworkers.start()\n## Stop workers after 120 seconds\nsleep(120)\nworkers.stop()\n\n", "tags": "", "url": "Messaging/index.html"},
		{"title": "Maestro Architecture", "text": "\nAccess to Github Pages\nCurrent architecture&#39;s limitations\n\nCreated for one big client, not in line with our current needs\nRequires re-training for every new data added\nTraining process hard to fully automized\nCategories and sub-categories are hard to define correctly\nMany problems arise when using datasets with overlapping meanings\n\nGuide lines for an evolution\n \n\nAvoid specific trainings whenever it&#39;s possible\nDo not require huge amount of data to start working\nBy default, no data leaks between projects\nUse categories as contextual helps instead of arbitrary silosEx: The question How many dead during the landing ?in the absolute, is not equal toHow many dead during the landing of the bay of pigs ?norHow many dead during the landing of the D-Day ?  \n\n \n\nBasically, the aim of the new architecture is to build a NLP search engine with the ability to tell if an information is present or not.\n\nArchitecture Overview\n \n\nImpacted Containers*\n\nNot Docker! In the C4 model, a container represents an application or a data store.A container is something that needs to be running in order for the overall software system to work. \n\nRasa(s)\n\n \nEach project has its own instance of Maestro, so we must also have one Rasa instance per Maestro with a dedicated RabbitMQ queue  \n\nEx: tbott-rpc.domain.project\n\nNeural Networks\n\nAs explained in Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks paper, while pretrained BERT model perform poorly for sentence embeddings, BERT models fine-tuned on sentence matching are slow and resource hungry.  \nSo to build an efficient search engine we must use two different models,\n\none for sentence encoding that will be fed to an efficient search index such as FAISS to quickly output candidates,\nand another one for sentence matching to be able to make the difference between a best match and a true match \n\nSentence Encoder\n\nEncode sentence as a contextual vector\nex: Universal Sentence Encoder Multilingual from Google\nUsed in Faiss Index to find best candidates\nTrained only once\nFollows SOTA improvements\n\nSentence Matching\n\nTells if two sentences are the same or not\nProvides  relevance score\nDo not use sentence encoder&#39;s vectors\nTrained only once\nFollows SOTA improvements\n\nMaestro\n\nMaestro is split in 3 parts for maintainability and efficiency.\nAll 3 containers are grouped within the same Pod\nA Pod is dedicated to a Target (ex: prod or test) attached to a Project within a DomainIn this doc referred as Domain.Project.Target\nFor redundancy and scalability, multiple Pods can share the same Domain.Project.env\nAll communications must be integrated within the Messaging library  \nMessages between containers use JSON format containing Numpy arrays.  \nCommunications within a Pod is done through ZeroMQ on 127.0.0.1-&gt; Containers within a Pod share the same network namespace, enabling the use of localhost address.-&gt; ZeroMQ is fast, flexible and is indifferent to the startup order of clients and servers.-&gt; more or less act as an IPC within the Pod  \nExemple of numpy serialisation with ZeroMQ: https://pyzmq.readthedocs.io/en/latest/serialization.html\n\n\n\nModules description\n\nFAISS\nFindReply\nAdmin\n\n", "tags": "", "url": "index.html"}
	]
}
